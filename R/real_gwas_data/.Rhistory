cut(neuroticism, breaks = seq(0, 24, by = 6))) %>%
summarise(sum(volunteer=="yes")/n()) %>% print(n = Inf)
Cowles %>% group_by(cut(extraversion, breaks = seq(0, 24, by = 6)),
cut(neuroticism, breaks = seq(0, 24, by = 6))) %>%
summarise(sum(volunteer=="yes")/n()) %>% print(n = Inf)
Anova(mod.cowles.1)
Cowles %>% group_by(sex) %>%
summarise(sum(volunteer=="yes")/n()) %>% print(n = Inf)
Cowles %>% group_by(sex) %>% drop_na(sex) %>%
summarise(sum(volunteer=="yes")/n()) %>% print(n = Inf)
Cowles %>% group_by(sex) %>% tidyr::drop_na(sex) %>%
summarise(sum(volunteer=="yes")/n()) %>% print(n = Inf)
plot(predictorEffects(mod.cowles.1,
xlevels = list(extraversion = seq(0, 24, by = 6),
neuroticism = seq(0, 24, by = 6))),
axes = list(y = list(type = "response")),
lines = list(multiline = TRUE), rows = 1, cols = 3)
mod.cowles.1 <- glm(volunteer == "No" ~ sex + neuroticism * extraversion,
data = Cowles, family = binomial)
mod.cowles.1 <- glm(volunteer ~ sex + neuroticism * extraversion,
data = Cowles, family = binomial)
mod.cowles.1 <- glm(factor(volunteer) ~ sex + neuroticism * extraversion,
data = Cowles, family = binomial)
car::Anova(mod.cowles.1)
anova(mod.cowles.1, test = "Chisq")
plot(predictorEffects(mod.cowles.1,
xlevels = list(extraversion = seq(0, 24, by = 6),
neuroticism = seq(0, 24, by = 6))),
axes = list(y = list(type = "response")),
lines = list(multiline = TRUE), rows = 1, cols = 3)
Cowles$nV = Cowles$volunteer == "No"
Cowles$nV
Cowles$volunteer
Cowles$nV = Cowles$volunteer == "no"
mod.cowles.1 <- glm(nV ~ sex + neuroticism * extraversion,
data = Cowles, family = binomial)
car::Anova(mod.cowles.1)
anova(mod.cowles.1, test = "Chisq")
plot(predictorEffects(mod.cowles.1,
xlevels = list(extraversion = seq(0, 24, by = 6),
neuroticism = seq(0, 24, by = 6))),
axes = list(y = list(type = "response")),
lines = list(multiline = TRUE), rows = 1, cols = 3)
# Personality effects on propensity to volunteer ####
library(car)
mod.cowles.1 <- glm(volunteer ~ sex + neuroticism * extraversion,
data = Cowles, family = binomial)
car::Anova(mod.cowles.1)
anova(mod.cowles.1, test = "Chisq")
plot(predictorEffects(mod.cowles.1,
xlevels = list(extraversion = seq(0, 24, by = 6),
neuroticism = seq(0, 24, by = 6))),
axes = list(y = list(type = "response")),
lines = list(multiline = TRUE), rows = 1, cols = 3)
plot(predictorEffects(mod.cowles.1, ~ neuroticism,
partial.residuals = TRUE), lattice = list(layout = c(4, 1)))
plot(predictorEffects(mod.cowles.1, ~ neuroticism,
partial.residuals = TRUE),
lattice = list(layout = c(4, 1)))
plot(predictorEffects(mod.cowles.1, ~ neuroticism,
partial.residuals = TRUE),
lattice = list(layout = c(4, 1)))
Cowles %>% group_by(cut(extraversion, breaks = seq(0, 24, by = 6)),
cut(neuroticism, breaks = seq(0, 24, by = 6))) %>%
summarise(sum(volunteer=="yes")/n()) %>% print(n = Inf)
plot(predictorEffects(mod.cowles.1,
xlevels = list(extraversion = seq(0, 24, by = 6),
neuroticism = seq(0, 24, by = 6))),
axes = list(y = list(type = "response")),
lines = list(multiline = TRUE), rows = 1, cols = 3)
Cowles %>% group_by(sex) %>%
summarise(sum(volunteer=="yes")/n()) %>% print(n = Inf)
# Replication Code for Fox and Weisberg, "Visualizing Fit and Lack of Fit
#   in Complex Regression Models with Predictor Effect Plots and Partial Residuals
## ------------------------------------------------------------------------
library("effects")
## ------------------------------------------------------------------------
m1 <- lm(log(infantMortality) ~ group * log(ppgdp), data = UN,
subset = rownames(UN) != "Equatorial Guinea")
summary(m1)
## Figure 1-a
plot(predictorEffect("group", m1,
transformation = list(link = log, inverse = exp),
xlevels = list(ppgdp = 10 ^ (2 : 5))),
lines = list(multiline = TRUE),
axes = list(
x = list(rotate = 45),
y = list(lab = "Infant Mortality",
ticks = list(at = 2 ^ (1 : 8)))
),
confint = list(style = "auto"))
## Figure 1-b
plot(predictorEffect("ppgdp", m1,
transformation = list(link = log, inverse = exp)),
lines = list(multiline = TRUE),
axes = list(
x = list(rotate = 45),
y = list(lab = "Infant Mortality",
ticks = list(at = 2 ^ (1 : 8)))
),
confint = list(style = "auto"))
## Figure 2
m2 <- lm(infantMortality ~ group * ppgdp, data = UN)
plot(predictorEffects(m2, ~ ppgdp, partial.residuals = TRUE),
axes = list(x = list(rotate = 25), y = list(lim = c(0, 150))),
id = list(n = 1))
## Figure 3
plot(predictorEffects(m1, ~ ppgdp, partial.residuals = TRUE),
axes = list(x = list(rotate = 25)))
## ------------------------------------------------------------------------
summary(Cowles)
library("car") # for Anova()
## ------------------------------------------------------------------------
mod.cowles.1 <- glm(volunteer ~ sex + neuroticism * extraversion,
data = Cowles, family = binomial)
summary(mod.cowles.1)
Anova(mod.cowles.1)
## Figure 4
plot(predictorEffects(mod.cowles.1,
xlevels = list(extraversion = seq(0, 24, by = 6),
neuroticism = seq(0, 24, by = 6))),
axes = list(y = list(type = "response")),
lines = list(multiline = TRUE),
rows = 1, cols = 3)
## Figure 5
plot(predictorEffects(mod.cowles.1,
~ neuroticism, partial.residuals = TRUE),
lattice = list(layout = c(4, 1)))
Cowles %>% group_by(cut(extraversion, breaks = seq(0, 24, by = 6)),
cut(neuroticism, breaks = seq(0, 24, by = 6))) %>%
summarise(sum(volunteer=="yes")/n()) %>% print(n = Inf)
## Figure 4
plot(predictorEffects(mod.cowles.1,
xlevels = list(extraversion = seq(0, 24, by = 6),
neuroticism = seq(0, 24, by = 6))),
axes = list(y = list(type = "response")),
lines = list(multiline = TRUE),
rows = 1, cols = 3)
### Effects plot with partial residuals #####
plot(predictorEffects(m2, ~ ppgdp, partial.residuals = TRUE),
axes = list(x = list(rotate = 25), y = list(lim = c(0, 150))),
layout = c(3,1),
id = list(n=1))
library(effects)
library(glmnet)      # function glmnet() can be used to fit ridge and lasso models
library(ISLR)
data(Hitters)
str(Hitters)
?Hitters
Hitters=na.omit(Hitters)      # remove missing values
# define x and y, as we cannot use y~x notation in glmnet
# x has to be a matrix and y a vector
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
# Ridge Regression
grid=10^seq(from = 10, to = -2, length=100)  # grid for lambda values
grid[95\]
grid[95]
grid[90]
grid[85]
grid[55]
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)    # alpha argument determines what type of model is fit
# alpha=0 corresponds to ridge
# associated with each value of lambda is a vector of ridge
# regression coefficients
dim(coef(ridge.mod)) # each row corresponds to a predictor (plus intercept),
ridge.mod$lambda[50] # =grid[50]: lambda = 11'498
coef(ridge.mod)[,50] # corresponding coefficient estimates
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # corresponding l_2-norm
ridge.mod$lambda[60] # =grid[60]: lambda = 705
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
# create training and test set
set.seed(1)                          #set random seed s.t. result can be reproduced
train=sample(1:nrow(x), nrow(x)/2)   # indices for training data
test=(-train)                        # indices we *leave out* for test data
test
y.test=y[test]
x.test=x[test,]
length(y.test)
length(y)
# conduct ridge regression on training data
ridge.mod = glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
par(mfrow=c(1,1))
plot(cv.out)
bestlamda=cv.out$lambda.min
bestlamda  # 212
ridge.mod$lambda
bestlambda
bestlamda
# evaluate test MSE for lambda=212
ridge.pred=predict(ridge.mod,s=bestlamda,newx=x[test,])
mean((ridge.pred-y.test)^2)       #96016
sqrt(139856.6)
sd(y)
# refit on the full data
out=glmnet(x,y,alpha=0)   # use default values for lambda
predict(out,type="coefficients",s=bestlamda)  # plug in lambda chosen by CV
[1:20,]
predict(out,type="coefficients",s=bestlamda)[1:20,]  # plug in lambda chosen by CV
# By using alpha=1 we can do similar things for the lasso:
# Conduct lasso regression on training data
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid, thresh=1e-12)
plot(lasso.mod)        # we can see from the coefficient plot that depending on lambda,
# By using alpha=1 we can do similar things for the lasso:
# Conduct lasso regression on training data
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid, thresh=1e-12)
plot(lasso.mod)        # we can see from the coefficient plot that depending on lambda,
cv.out = cv.glmnet(x[train,],y[train],alpha=1)     #perform CV on training set
par(mfrow=c(1,1))
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam  # 35.3
# evaluate test MSE for lambda=35.3
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)         # 102'442
data(Hitters)
str(Hitters)
?Hitters
Hitters=na.omit(Hitters)      # remove missing values
# define x and y, as we cannot use y~x notation in glmnet
# x has to be a matrix and y a vector
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
# Ridge Regression
grid=10^seq(from = 10, to = -2, length=100)  # grid for lambda values
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)    # alpha argument determines what type of model is fit
# alpha=0 corresponds to ridge
# associated with each value of lambda is a vector of ridge
# regression coefficients
dim(coef(ridge.mod)) # each row corresponds to a predictor (plus intercept),
ridge.mod$lambda[50] # =grid[50]: lambda = 11'498
coef(ridge.mod)[,50] # corresponding coefficient estimates
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # corresponding l_2-norm
ridge.mod$lambda[60] # =grid[60]: lambda = 705
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
# create training and test set
set.seed(1)                          #set random seed s.t. result can be reproduced
train=sample(1:nrow(x), nrow(x)/2)   # indices for training data
test=(-train)                        # indices we *leave out* for test data
y.test=y[test]
x.test=x[test,]
# conduct ridge regression on training data
ridge.mod = glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
# choose lambda via cross validation (use built-in cross validation function cv.glmnet())
?cv.glmnet   # conducts 10 fold cross validation by default
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
par(mfrow=c(1,1))
plot(cv.out)
bestlamda=cv.out$lambda.min
bestlamda  # 212
# evaluate test MSE for lambda=212
ridge.pred=predict(ridge.mod,s=bestlamda,newx=x[test,])
mean((ridge.pred-y.test)^2)       #96016
# refit on the full data
out=glmnet(x,y,alpha=0)   # use default values for lambda
predict(out,type="coefficients",s=bestlamda)[1:20,]  # plug in lambda chosen by CV
# By using alpha=1 we can do similar things for the lasso:
# Conduct lasso regression on training data
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid, thresh=1e-12)
plot(lasso.mod)        # we can see from the coefficient plot that depending on lambda,
cv.out = cv.glmnet(x[train,],y[train],alpha=1)     #perform CV on training set
par(mfrow=c(1,1))
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam  # 35.3
# evaluate test MSE for lambda=35.3
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)         # 102'442
# refit on the full data
out=glmnet(x,y,alpha=1)   # use default values for lambda
predict(out,type="coefficients",s=bestlam)[1:20,]  # plug in lambda chosen by CV
predict(out,type="coefficients",s=bestlamda)[1:20,]  # plug in lambda chosen by CV
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
par(mfrow=c(1,1))
plot(cv.out)
bestlamda=cv.out$lambda.min
bestlamda  # 212
# evaluate test MSE for lambda=212
ridge.pred=predict(ridge.mod,s=bestlamda,newx=x[test,])
mean((ridge.pred-y.test)^2)       #96016
# refit on the full data
out=glmnet(x,y,alpha=0)   # use default values for lambda
predict(out,type="coefficients",s=bestlamda)[1:20,]  # plug in lambda chosen by CV
# By using alpha=1 we can do similar things for the lasso:
# Conduct lasso regression on training data
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid, thresh=1e-12)
plot(lasso.mod)        # we can see from the coefficient plot that depending on lambda,
cv.out = cv.glmnet(x[train,],y[train],alpha=1)     #perform CV on training set
par(mfrow=c(1,1))
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam  # 35.3
# evaluate test MSE for lambda=35.3
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)         # 102'442
# refit on the full data
out=glmnet(x,y,alpha=1)   # use default values for lambda
predict(out,type="coefficients",s=bestlam)[1:20,]  # plug in lambda chosen by CV
y=log(Hitters$Salary)
# Ridge Regression
grid=10^seq(from = 10, to = -2, length=100)  # grid for lambda values
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)    # alpha argument determines what type of model is fit
# alpha=0 corresponds to ridge
# associated with each value of lambda is a vector of ridge
# regression coefficients
dim(coef(ridge.mod)) # each row corresponds to a predictor (plus intercept),
ridge.mod$lambda[50] # =grid[50]: lambda = 11'498
coef(ridge.mod)[,50] # corresponding coefficient estimates
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # corresponding l_2-norm
ridge.mod$lambda[60] # =grid[60]: lambda = 705
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
# create training and test set
set.seed(1)                          #set random seed s.t. result can be reproduced
train=sample(1:nrow(x), nrow(x)/2)   # indices for training data
test=(-train)                        # indices we *leave out* for test data
y.test=y[test]
x.test=x[test,]
# conduct ridge regression on training data
ridge.mod = glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
# choose lambda via cross validation (use built-in cross validation function cv.glmnet())
?cv.glmnet   # conducts 10 fold cross validation by default
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
par(mfrow=c(1,1))
plot(cv.out)
bestlamda=cv.out$lambda.min
bestlamda  # 212
# evaluate test MSE for lambda=212
ridge.pred=predict(ridge.mod,s=bestlamda,newx=x[test,])
mean((ridge.pred-y.test)^2)       #96016
# refit on the full data
out=glmnet(x,y,alpha=0)   # use default values for lambda
predict(out,type="coefficients",s=bestlamda)[1:20,]  # plug in lambda chosen by CV
# By using alpha=1 we can do similar things for the lasso:
# Conduct lasso regression on training data
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid, thresh=1e-12)
plot(lasso.mod)        # we can see from the coefficient plot that depending on lambda,
cv.out = cv.glmnet(x[train,],y[train],alpha=1)     #perform CV on training set
par(mfrow=c(1,1))
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam  # 35.3
# evaluate test MSE for lambda=35.3
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)         # 102'442
# refit on the full data
out=glmnet(x,y,alpha=1)   # use default values for lambda
predict(out,type="coefficients",s=bestlam)[1:20,]  # plug in lambda chosen by CV
# lasso has substantial advantage over ridge regression because
sd(y0)
sd(y)
hist(y)
hist(exp(y))
hist(y)
#1# Loading and preparing the input data
library(BGLR); data(mice);
Y<-mice.pheno; X<-mice.X
Y[is.na(Y$Biochem.Age), "Biochem.Age"] = median(Y$Biochem.Age, na.rm = TRUE) #median impute age
y<-Y$Obesity.BMI; y<-scale(y,center=TRUE,scale=TRUE)
#2# Setting the linear predictor
ETA<-list(  FIXED=list(~factor(GENDER)+factor(Litter)+Biochem.Age,
data=Y,model="FIXED"),
CAGE=list(~factor(cage),data=Y, model="BRR"),
MRK=list(X=X,  model="BL")
)
#3# Fitting the model
set.seed(100)
fm<-BGLR(y=y,ETA=ETA, nIter=11000, burnIn=1000)
#1# Loading and preparing the input data
library(BGLR); data(mice);
Y<-mice.pheno; X<-mice.X
Y[is.na(Y$Biochem.Age), "Biochem.Age"] = median(Y$Biochem.Age, na.rm = TRUE) #median impute age
y<-Y$Obesity.BMI; y<-scale(y,center=TRUE,scale=TRUE)
#2# Setting the linear predictor
ETA<-list(  FIXED=list(~factor(GENDER)+Biochem.Age+factor(Litter)+,
setwd("~/.julia/dev/HighDimMixedModels/R)
#1# Loading and preparing the input data
library(BGLR); data(mice);
Y<-mice.pheno; X<-mice.X
Y[is.na(Y$Biochem.Age), "Biochem.Age"] = median(Y$Biochem.Age, na.rm = TRUE) #median impute age
setwd("~/.julia/dev/HighDimMixedModels/R")
setwd("~/.julia/dev/HighDimMixedModels/R")
#1# Loading and preparing the input data
library(BGLR); data(mice);
Y<-mice.pheno; X<-mice.X
Y[is.na(Y$Biochem.Age), "Biochem.Age"] = median(Y$Biochem.Age, na.rm = TRUE) #median impute age
y<-Y$Obesity.BMI; y<-scale(y,center=TRUE,scale=TRUE)
#2# Setting the linear predictor
ETA<-list(  FIXED=list(~factor(GENDER)+Biochem.Age+factor(Litter)+,
#3# Fitting the model
set.seed(100)
#1# Loading and preparing the input data
library(BGLR); data(mice);
Y<-mice.pheno; X<-mice.X
Y[is.na(Y$Biochem.Age), "Biochem.Age"] = median(Y$Biochem.Age, na.rm = TRUE) #median impute age
y<-Y$Obesity.BMI; y<-scale(y,center=TRUE,scale=TRUE)
#2# Setting the linear predictor
ETA<-list(  FIXED=list(~factor(GENDER)+Biochem.Age+factor(Litter),
data=Y,model="FIXED"),
CAGE=list(~factor(cage),data=Y, model="BRR"),
MRK=list(X=X,  model="BL")
)
#3# Fitting the model
#Move to correct folder so that files automatically created by BGLR are saved in right place
setwd("~/.julia/dev/HighDimMixedModels/R/real_gwas_data/")
#1# Loading and preparing the input data
library(BGLR); data(mice);
Y<-mice.pheno; X<-mice.X
Y[is.na(Y$Biochem.Age), "Biochem.Age"] = median(Y$Biochem.Age, na.rm = TRUE) #median impute age
y<-Y$Obesity.BMI; y<-scale(y,center=TRUE,scale=TRUE)
#2# Setting the linear predictor
ETA<-list(  FIXED=list(~factor(GENDER)+Biochem.Age+factor(Litter),
data=Y,model="FIXED"),
CAGE=list(~factor(cage),data=Y, model="BRR"),
MRK=list(X=X,  model="BL")
)
#3# Fitting the model
#Move to correct folder so that files automatically created by BGLR are saved in right place
setwd("~/.julia/dev/HighDimMixedModels/R/real_gwas_data/")
set.seed(100)
fm<-BGLR(y=y,ETA=ETA, nIter=11000, burnIn=1000)
save(fm,file="./fm.rda")
getwd()
#4# Trace plots
list.files()
#1# Estimated Marker Effects & posterior SDs
load("~/.julia/dev/HighDimMixedModels/R/fm.rda")
save(fm,file="./fm.rda")
#1# Estimated Marker Effects & posterior SDs
load("~/.julia/dev/HighDimMixedModels/R/fm.rda")
#1# Estimated Marker Effects & posterior SDs
load("./fm.rda")
bHat<- fm$ETA$MRK$b
SD.bHat<- fm$ETA$MRK$SD.b
plot(abs(bHat), ylab="Estimated Squared-Marker Effect",
type="o",cex=.5,col="red",main="Marker Effects",
xlab="Marker")
points(abs(bHat),cex=0.5,col="blue")
#2# Predictions
# Genomic Prediction
gHat<-X%*%fm$ETA$MRK$b + ifelse(Y$GENDER == "M", 1, 0)*fm$ETA$FIXED$b[1]
plot(fm$y~gHat,ylab="Phenotype",
xlab="Predicted Genomic Value", col=Y$Litter, pch = as.integer(Y$GENDER), cex=0.5,
main="Predicted Genomic Values Vs Phenotypes",
xlim=range(gHat),ylim=range(fm$y));
abline(0, 1, col = "yellow")
gHat<- fm$mu + fixed_X%*%fm$ETA$FIXED$b + X%*%fm$ETA$MRK$b
#2# Predictions
# Genomic Prediction
fixed_X = model.matrix( ~ factor(GENDER) +  Biochem.Age + factor(Litter), data=Y)[-1,]
gHat<- fm$mu + fixed_X%*%fm$ETA$FIXED$b + X%*%fm$ETA$MRK$b
#2# Predictions
# Genomic Prediction
fixed_X = model.matrix( ~ factor(GENDER) +  Biochem.Age + factor(Litter), data=Y)[,-1]
gHat<- fm$mu + fixed_X%*%fm$ETA$FIXED$b + X%*%fm$ETA$MRK$b
plot(fm$y~gHat,ylab="Phenotype",
xlab="Predicted Genomic Value", col=Y$Litter, pch = as.integer(Y$GENDER), cex=0.5,
main="Predicted Genomic Values Vs Phenotypes",
xlim=range(gHat),ylim=range(fm$y));
abline(0, 1, col = "yellow")
plot(fm$y~gHat,ylab="Phenotype",
xlab="Predicted Genomic Value", cex=0.5,
main="Predicted Genomic Values Vs Phenotypes",
xlim=range(gHat),ylim=range(fm$y));
abline(0, 1, col = "yellow")
abline(0, 1, col = "blue")
legend(col = unique(Y$Litter), pch = 1, legend = unique(Y$Litter), x = "topleft")
#3# Godness of fit and related statistics
fm$fit
fm$varE # compare to var(y)
#4# Trace plots
list.files()
# Residual variance
varE<-scan("varE.dat")
plot(varE,type="o",col=2,cex=.5,
ylab=expression(sigma[epsilon]^2),
xlab="Sample",main="Residual Variance");
abline(h=fm$varE,col=4,lwd=2);
abline(v=fm$burnIn/fm$thin,col=4)
# lambda (regularization parameter of the Bayesian LASSO)
lambda<-scan("ETA_MRK_lambda.dat")
plot(lambda,type="o",col=2,cex=.5,
xlab="Sample",ylab=expression(lambda),
main="Regularization parameter");
abline(h=fm$ETA$MRK$lambda,col=4,lwd=2);
abline(v=fm$burnIn/fm$thin,col=4)
gHat<- fm$mu + fixed_X%*%fm$ETA$FIXED$b + X%*%fm$ETA$MRK$b
fm$varE
fm$ETA$CAGE$varB
fm$ETA$CAGE$b
fm$ETA$CAGE$b %>% length()
library(dplyr)
fm$ETA$CAGE$b %>% length()
length(unique(Y$cage))
dif(unique(Y$cage), names(fm$ETA$CAGE$b) )
setdiff(unique(Y$cage), names(fm$ETA$CAGE$b) )
unique(Y$cage)
unique(Y$cage)
fm$yHat
gHat
yHat
cbind(fm$yHat, gHat)
var(fm$y - gHat)
var(fm$y - fm$yHat)
yhat
fm$yHat
